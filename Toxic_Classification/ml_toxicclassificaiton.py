"""ML ToxicClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FtGnZnfmnV4V4ejk11l-91S0cvKC419l?usp=sharing

# **IMPORT** **LIBRARY**
"""

import pandas as pd
import numpy as np
import torch
from datasets import Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)

# **PATH VARIABLES**
MODEL_NAME = "xlm-roberta-base"

# Datasete Link (Got it from Hugging Faces)
PARQUET_FILE_PATH = "hf://datasets/textdetox/multilingual_toxicity_dataset/data/en-00000-of-00001.parquet"

TEXT_COLUMN = 'text'  
LABEL_COLUMN_ORIGINAL = 'toxic' 

# Untuk klasifikasi (toxic/non-toxic)
NUM_LABELS = 2 

# Max length untuk tokenizer
MAX_LENGTH = 128 

OUTPUT_DIR = "./toxic-model-xlm-roberta"
FINAL_MODEL_SAVE_PATH = "./toxic-xlm-roberta-final"

# **PREPROCESSING**
df = pd.read_parquet(PARQUET_FILE_PATH)

# Ganti nama kolom label menjadi 'labels' agar sesuai dengan ekspektasi Trainer
if LABEL_COLUMN_ORIGINAL in df.columns:
    df = df.rename(columns={LABEL_COLUMN_ORIGINAL: 'labels'})
else:
    # Jika kolom label asli tidak ditemukan, coba cek apakah sudah ada 'labels'
    if 'labels' not in df.columns:
        raise ValueError(f"Kolom label asli '{LABEL_COLUMN_ORIGINAL}' tidak ditemukan dan tidak ada kolom 'labels'.")

df.head()

# Konversi ke Hugging Face Dataset
dataset = Dataset.from_pandas(df)

# Split - Train
dataset_splits = dataset.train_test_split(test_size=0.2, seed=42) 
train_dataset_raw = dataset_splits['train']
eval_dataset_raw = dataset_splits['test']

# **TOKENIZER**
# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
def preprocess_function(examples):
    return tokenizer(
        examples[TEXT_COLUMN],
        truncation=True,
        padding='max_length',
        max_length=MAX_LENGTH
    )

train_dataset_tokenized = train_dataset_raw.map(preprocess_function, batched=True)
eval_dataset_tokenized = eval_dataset_raw.map(preprocess_function, batched=True)

# Set format agar bisa digunakan oleh PyTorch
train_dataset_tokenized.set_format("torch")
eval_dataset_tokenized.set_format("torch")

# **CLASS WEIGHTS**
labels_for_weights = df['labels'].values
class_counts = np.bincount(labels_for_weights)

if len(class_counts) < NUM_LABELS:
    full_class_counts = np.zeros(NUM_LABELS, dtype=int)
    full_class_counts[:len(class_counts)] = class_counts
    class_counts = full_class_counts
    class_counts[class_counts == 0] = 1 # Hindari pembagian dengan nol, namun perlu ditinjau jika ada kelas yang benar-benar hilang

if np.any(class_counts == 0):
     print("Peringatan: Ada kelas dengan jumlah sampel 0. Bobot untuk kelas ini mungkin tidak optimal.")

class_weights_values = sum(class_counts) / class_counts # Bobot invers proporsional
class_weights = torch.tensor(class_weights_values, dtype=torch.float)
print(f"Bobot kelas yang dihitung: {class_weights}")

# **MODEL**
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)

# **CUSTOM TRAINER**
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        current_device = logits.device
        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(current_device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

# **EVALUATION**
# Evaluation
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='binary', zero_division=0
    )
    acc = accuracy_score(labels, preds)
    return {
        "accuracy": acc,
        "f1": f1,
        "precision": precision,
        "recall": recall,
    }
    
# **TRAINING**

# Training
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3, 
    per_device_train_batch_size=16, 
    per_device_eval_batch_size=32,  
    gradient_accumulation_steps=2,  
    learning_rate=2e-5,             
    weight_decay=0.01,              
    warmup_ratio=0.1,               
    logging_steps=50,             
    eval_strategy="epoch",        
    save_strategy="epoch",        
    save_total_limit=2,            
    load_best_model_at_end=True,    
    metric_for_best_model="f1",    
    greater_is_better=True,         
    fp16=torch.cuda.is_available(), 
    seed=42,
)

# Trainer
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset_tokenized,
    eval_dataset=eval_dataset_tokenized,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)

trainer.train()

# Save the Best Model
trainer.save_model(FINAL_MODEL_SAVE_PATH)
tokenizer.save_pretrained(FINAL_MODEL_SAVE_PATH)

