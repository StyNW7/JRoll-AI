# -*- coding: utf-8 -*-
"""ML RecommendationSystemAnime.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_aQ9UDUJ-9tNp79XctqG64wVy3or9Y1m

# **IMPORT** **LIBRARY**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score
import nltk
from datetime import datetime
import pickle
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from scipy.sparse import hstack, csr_matrix
import random

"""# **LOAD DATASET**"""

# Download necessary NLTK data
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Suppress warnings
warnings.filterwarnings('ignore')

# Load dataset
df = pd.read_csv('./dataset_anime/anime-dataset-2023.csv')
ratings = pd.read_csv('./dataset_anime/users-score-2023.csv')

# print(f"anime dataset: {df.shape} ")
# print(f"user score dataset: {ratings.shape}")

"""# **EDA (EXPLORE DATA ANALYSIS)**"""

# df.head()

# df.info()

# missing_values = df.isnull().sum().sort_values(ascending=False)
# missing_values.head()

"""Cek unknown data"""

# for col in df.columns:
#     if df[col].dtype == object:  # Only check object columns
#         unknown_mask = df[col] == 'UNKNOWN'
#         if unknown_mask.any():
#             print(f"Column '{col}' has {unknown_mask.sum()} 'UNKNOWN' values")

"""Fill the missing Values"""

# Convert Score to numeric
df['Score'] = pd.to_numeric(df['Score'], errors='coerce')

# Convert Episodes to numeric
df['Episodes'] = pd.to_numeric(df['Episodes'].replace('UNKNOWN', np.nan), errors='coerce')

# Convert Favorites, Members, and Scored By to numeric
numeric_cols = ['Favorites', 'Popularity', 'Members', 'Scored By']
for col in numeric_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col].replace('UNKNOWN', np.nan), errors='coerce')

# Fill missing episodes with median by Type
median_episodes = df.groupby('Type')['Episodes'].transform('median')
df['Episodes'].fillna(median_episodes, inplace=True)
# For any remaining NaN episodes, fill with overall median
df['Episodes'].fillna(df['Episodes'].median(), inplace=True)

# Drop rows with missing scores
df.dropna(subset=['Score'], inplace=True)
df = df[(df['Genres'] != 'UNKNOWN') & (df['Synopsis'] != 'UNKNOWN') & (df['Type'] != "UNKNOWN")]
drop_cols = ['Premiered', 'Licensors', 'Rank', 'Other name']
df.drop(drop_cols, axis=1, inplace=True)

# print(f"Dataset shape after handling critical missing values: {df.shape}")

df.replace('UNKNOWN', pd.NA, inplace=True)
df['Studios'].fillna('No data', inplace=True)
df['Producers'].fillna('No data', inplace=True)
df['Rating'].fillna("NO data", inplace = True)

# Make sure English name is filled if available
if "English name" in df.columns:
    # If English name is empty but Name exists, use Name
    mask = df["English name"].isna() | (df["English name"] == "UNKNOWN")
    df.loc[mask, "English name"] = df.loc[mask, "Name"]

# Check again Unknown Value
# for col in df.columns:
#     if df[col].dtype == object:  # Only check object columns
#         unknown_mask = df[col] == 'UNKNOWN'
#         if unknown_mask.any():
#             print(f"Column '{col}' has {unknown_mask.sum()} 'UNKNOWN' values")

# print(f"Dataset shape after handling critical missing values: {df.shape}")

# missing_values = df.isnull().sum().sort_values(ascending=False)
# missing_values.head()

# Basic EDA
# print("\n--- Basic Exploratory Data Analysis ---")
# print(f"Score distribution statistics:\n{df['Score'].describe()}")

# plt.figure(figsize=(10, 6))
# sns.histplot(df['Score'], kde=True)
# plt.title('Distribution of Anime Scores')
# plt.show()

df.columns

"""# Feature Selection"""

feature_columns = [
    'anime_id', 'Name', 'English name', 'Score', 'Genres', 'Synopsis',
       'Type', 'Episodes', 'Aired', 'Status', 'Producers', 'Studios', 'Source',
       'Duration', 'Rating', 'Popularity', 'Favorites', 'Scored By', 'Members',
       'Image URL'
]

"""Use only columns that exist in the dataset"""

available_columns = [col for col in feature_columns if col in df.columns]
animes = df[available_columns].copy()
# print(f"Selected features: {animes.columns.tolist()}")

"""# Text Preprocessing"""

def preprocess_text(text):
    if not isinstance(text, str) or text == "UNKNOWN":
        return ""
    # Convert to lowercase
    text = text.lower()
    # Remove special characters
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    # Tokenize
    words = text.split()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]
    # Stemming
    ps = PorterStemmer()
    words = [ps.stem(word) for word in words]
    # Join words back
    return ' '.join(words)

# Apply text preprocessing
# print("\n--- Text Preprocessing ---")
text_columns = ['Synopsis', 'Type']
for col in text_columns:
    if col in animes.columns:
        animes[f'processed_{col.lower()}'] = animes[col].astype(str).apply(preprocess_text)

# print("Text preprocessing completed")

"""# Vectorization using TF-IDF"""

# TF-IDF for Genres
tfidf_genres = TfidfVectorizer(tokenizer=lambda x: x.split(', '))
genres_vec = tfidf_genres.fit_transform(animes['Genres'])


# TF-IDF for Synopsis
tfidf_synopsis = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 2),
    sublinear_tf=True
)
synopsis_vec = tfidf_synopsis.fit_transform(animes['processed_synopsis'])

"""# Normalization"""

# Normalize Score
# print("Normalizing Score...")
scaler = MinMaxScaler()
score_scaled = scaler.fit_transform(animes[['Score']])
score_sparse = csr_matrix(score_scaled)  # convert to sparse

# One-hot encode Type and Premiered
# print("One-hot Encoding Type and Premiered...")
encoder = OneHotEncoder(handle_unknown='ignore')
cat_encoded = encoder.fit_transform(animes[['Type']])

# print("Normalizing Popularity...")
popularity_scaled = scaler.fit_transform(animes[['Popularity']])
popularity_sparse = csr_matrix(popularity_scaled)

"""# Combine all feature vectors"""

final_vectors = hstack([genres_vec, synopsis_vec, score_sparse, cat_encoded, popularity_sparse])
# print(f"Final vector shape: {final_vectors.shape}")

"""Calculate similarity matrix"""

similarity = cosine_similarity(final_vectors)
# print(f"Similarity matrix shape: {similarity.shape}")

"""Content-Based Filtering"""

import difflib

def recommend_anime(title, top_n=5, similarity_matrix=similarity, anime_df=animes):
    title = title.lower()

    # Gabungkan pencarian di Name dan English name
    match = anime_df[anime_df['Name'].str.lower() == title]
    if match.empty:
        match = anime_df[anime_df['English name'].str.lower() == title]

    # Jika masih tidak ditemukan, cari yang paling mirip
    if match.empty:
        all_titles = anime_df['Name'].str.lower().tolist()
        closest_matches = difflib.get_close_matches(title, all_titles, n=1)
        if not closest_matches:
            print(f"No anime found with title similar to '{title}'")
            return pd.DataFrame()
        print(f"No exact match found. Using closest match: {closest_matches[0]}")
        match = anime_df[anime_df['Name'].str.lower() == closest_matches[0]]

    idx = match.index[0]

    # Hitung similarity dan ambil top N
    sim_scores = list(enumerate(similarity_matrix[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[:top_n]
    top_indices = [i[0] for i in sim_scores]

    # Ambil anime rekomendasi
    recommendations = anime_df.iloc[top_indices].copy()
    recommendations['similarity_score'] = [i[1] for i in sim_scores]

    results = []
    for _, row in recommendations.iterrows():
        # Extract year from 'Aired' column
        aired = str(row.get('Aired', ''))
        year = "Unknown"
        match = re.search(r'\d{4}', aired)
        if match:
            year = match.group(0)

        # Ambil genre list
        genres_raw = row.get('Genres', '')
        genres = [g.strip() for g in genres_raw.split(',')] if isinstance(genres_raw, str) else []

        # Tentukan apakah anime ini baru
        is_new = False
        try:
            is_new = int(year) >= 2022
        except:
            pass

        results.append({
            "title": row['Name'],
            "image": row.get('Image URL', "/Images/Card/japan.png"),
            "rating": str(row.get('Score', "N/A")),
            "year": year,
            "episodes": int(row['Episodes']) if pd.notnull(row['Episodes']) else 0,
            "genres": row['Genres'].split(', ') if pd.notnull(row['Genres']) else [],
            "isNew": '2024' in year or '2025' in year,
            "similarity_score": float(row['similarity_score']) if 'similarity_score' in row else None
        })

    return results

""" Item-Based Collaborative Filtering"""

def collaborative_recommendations(user_favorites=None, top_n=5, similarity_matrix=similarity, anime_df=animes):

    if not user_favorites or len(user_favorites) == 0:
        print("No user favorites provided.")
        return pd.DataFrame()

    # Normalize and index user favorites
    user_favorites = [fav.strip().lower() for fav in user_favorites]
    favorite_indices = []
    for fav in user_favorites:
        match = anime_df[anime_df['Name'].str.lower() == fav]
        if not match.empty:
            favorite_indices.append(match.index[0])

    if not favorite_indices:
        print("None of the favorite titles matched the dataset.")
        return pd.DataFrame()

    # Compute average similarity score across user favorites
    scores = []
    for idx in range(len(anime_df)):
        if idx in favorite_indices:
            continue  # Skip already liked anime
        avg_sim = np.mean([similarity_matrix[idx, fav_idx] for fav_idx in favorite_indices])
        scores.append((idx, avg_sim))

    # Sort by collaborative score
    scores.sort(key=lambda x: x[1], reverse=True)
    top_indices = [i[0] for i in scores[:top_n]]

    # Prepare recommendations
    recommendations = anime_df.iloc[top_indices].copy()
    recommendations['collab_score'] = [i[1] for i in scores[:top_n]]

    # Display columns
    display_columns = ['Name', 'Score', 'Genres', 'Type', 'Episodes', 'collab_score']
    if 'English name' in anime_df.columns:
        display_columns.insert(1, 'English name')
    for col in ['Studios', 'Source', 'Rating', 'Popularity']:
        if col in anime_df.columns:
            display_columns.insert(-1, col)

    return recommendations[display_columns]

"""User-Based Collaborative Filtering"""

popular_anime = ratings['anime_id'].value_counts()[lambda x: x >= 50000].index
filtered_ratings = ratings[ratings['anime_id'].isin(popular_anime)]
user_item_matrix = filtered_ratings.pivot_table(index='user_id', columns='anime_id', values='rating')

# 3. Hitung similarity antar anime (item-based CF)
# Gantilah NaN dengan 0 agar bisa dihitung similarity
item_similarity = cosine_similarity(user_item_matrix.fillna(0).T)
item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)

# 4. Fungsi rekomendasi berdasarkan anime yang disukai user
def recommend_collaborative(user_favorites, top_n=5, anime_df=animes):
    fav_ids = []
    for title in user_favorites:
        match = anime_df[anime_df['Name'].str.lower() == title.lower()]
        if not match.empty:
            fav_ids.append(match.iloc[0]['anime_id'])

    if not fav_ids:
        print("Anime favorit tidak ditemukan.")
        return []

    scores = pd.Series(dtype=float)
    for anime_id in fav_ids:
        similar_scores = item_similarity_df.get(anime_id)
        if similar_scores is not None:
            scores = scores.add(similar_scores, fill_value=0)

    scores = scores.drop(labels=fav_ids, errors='ignore')
    top_recs = scores.sort_values(ascending=False).head(top_n)

    recommendations = anime_df[anime_df['anime_id'].isin(top_recs.index)].copy()
    recommendations['collab_score'] = recommendations['anime_id'].map(top_recs)

    results = []
    for _, row in recommendations.iterrows():
        # Extract year from 'Aired' column if possible
        aired = str(row.get('Aired', ''))
        year = "Unknown"
        if aired and aired != 'nan':
            import re
            match = re.search(r'(\d{4})', aired)
            if match:
                year = match.group(1)

        results.append({
            "title": row['Name'],
            "image": row.get('Image URL', "/Images/Card/japan.png"),  # fallback image
            "rating": str(row.get('Score', "N/A")),
            "year": year,
            "episodes": int(row['Episodes']) if pd.notnull(row['Episodes']) else 0
        })

    return results


# def display_recommendations(recommendations, user_favorites=None):

#     if recommendations.empty:
#         return

#     print("\n========== ANIME RECOMMENDATIONS ==========\n")

#     if user_favorites and len(user_favorites) > 0:
#         print("Based on your favorites:")
#         for i, fav in enumerate(user_favorites, 1):
#             print(f"  {i}. {fav}")
#         print("")


#     for i, (_, row) in enumerate(recommendations.iterrows(), 1):
#         print(f"{i}. {row['Name']}")
#         print(f"   Genre: {row['Genres']}")
#         print(f"   Type: {row['Type']} | Episodes: {row['Episodes']} | Score: {row['Score']:.2f}")
#         if 'collab_score' in row:
#            print(f"   Collaborative Score: {row['collab_score']:.4f}")
#         elif 'similarity_score' in row:
#            print(f"   Similarity Score: {row['similarity_score']:.4f}")
#         print("-" * 50)

# """TRY RECOMMENDER SYSTEM"""

# CB_recommendations = recommend_anime("Naruto")
# display_recommendations(CB_recommendations)

# user_favorites = ["Hunter x Hunter", "Naruto: Shippuuden"]
# try:
#     recommend = recommend_collaborative(user_favorites=user_favorites)
#     display_recommendations(recommend, user_favorites)
# except Exception as e:
#     print(f"Error during hybrid recommendation: {e}")

# anime_names = ["One Piece", "Naruto: Shippuuden", "Naruto"]
# with open('anime_names.pkl', 'wb') as f:
#     pickle.dump(anime_names, f)

# with open('anime_names.pkl', 'rb') as f:
#     anime_names = pickle.load(f)

# def anime_recommendation_system():
#     """
#     Simple interactive UI for anime recommendations
#     """
#     print("\n===== ANIME RECOMMENDATION SYSTEM =====")
#     print("1. Get recommendations based on anime title")
#     print("2. Get collaborative filtering")
#     print("3. Exit")

#     choice = input("\nEnter your choice (1-3): ")

#     if choice == '1':
#         title = input("Enter anime title: ")
#         try:
#             recommendations = recommend_anime(title)
#             display_recommendations(recommendations)
#         except Exception as e:
#             print(f"Error: {e}")

#     elif choice == '2':
#         random.shuffle(anime_names)
#         favorites = anime_names[1:]
#         try:
#             recommendations = recommend_collaborative(user_favorites=favorites)
#             display_recommendations(recommendations, anime_names)
#         except Exception as e:
#             print(f"Error: {e}")

#     elif choice == '3':
#         return False

#     else:
#         print("Invalid choice. Please try again.")

#     return True

# """# DEMO"""

# print("\n--- Recommendation Demo ---")
# def run_interactive_system():
#     """Run the interactive recommendation system"""
#     print("\nWelcome to the Anime Recommendation System!")
#     while anime_recommendation_system():
#         pass
#     print("Thank you for using the Anime Recommendation System!")


# run_interactive_system()

"""# **Kesimpulan**

# 1. Content-Based Filtering
* Score disebut: Similarity Score

* Metode: Menghitung kemiripan berdasarkan metadata seperti genre, synopsis, dll.

Contoh skor:

Naruto Shippuuden → 0.8124

Boruto → 0.8009

**Karakteristik:**

* Skor biasanya < 1 karena berdasarkan cosine similarity terhadap fitur konten.

* Rekomendasi sangat mirip secara tema atau deskripsi konten.

* Tidak memperhitungkan opini pengguna lain.

# 2. Item-Based Collaborative Filtering
* Score disebut: Collaborative Score

* Metode: Menggunakan kesamaan pola rating antar anime (berbasis item-item similarity) dari banyak user.

Contoh skor:

Hunter x Hunter → 0.7063

One Piece → 0.6863

**Karakteristik:**

* Skor bisa < 1, tetap dalam rentang 0–1 (karena menggunakan cosine similarity antar kolom matrix user-item).

* Cocok untuk menemukan anime yang dinilai mirip oleh banyak user (walaupun genre berbeda).

* Lebih robust terhadap informasi deskriptif karena berbasis data perilaku.

# 3. User-Based Collaborative Filtering
- Score disebut: Collaborative Score

- Metode: Menghitung skor rekomendasi dari user-user yang memiliki pola preferensi serupa.

Contoh skor:

Naruto Shippuuden → 1.1992

Bleach → 1.0938

**Karakteristik:**

- Skor bisa > 1, karena hasil agregat dari skor rating user-user serupa.

- Rekomendasi sangat personal, tergantung kesamaan dengan user lain.
"""
